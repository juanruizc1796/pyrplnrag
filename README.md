# Chatbot JurÃ­dico RAG Â· Ley 769 de 2002 (CÃ³digo Nacional de TrÃ¡nsito)

Este proyecto implementa un **chatbot legal especializado** en la Ley 769 de 2002 (CÃ³digo Nacional de TrÃ¡nsito de Colombia) utilizando tÃ©cnicas modernas de **Procesamiento de Lenguaje Natural (PLN)**, **RAG (Retrieval-Augmented Generation)** y **modelos de lenguaje abiertos (LLMs)**.  
El objetivo es permitir que cualquier usuario pueda realizar preguntas sobre normativa de trÃ¡nsito y obtener respuestas **precisas, explicadas en lenguaje claro y siempre citando los artÃ­culos relevantes de la ley**.

El proyecto forma parte del curso de PLN, cumpliendo los lineamientos de creaciÃ³n de un sistema interactivo que procese lenguaje natural, recupere informaciÃ³n y genere respuestas interpretables. TambiÃ©n se incluye un componente voluntario de **observabilidad** para analizar el desempeÃ±o del sistema durante su uso.

---

## ğŸš€ Objetivo del Proyecto

Construir un **agente conversacional inteligente** capaz de responder preguntas jurÃ­dicas basadas exclusivamente en la Ley 769 de 2002.  
El sistema debe:

1. Procesar el texto oficial de la ley.
2. Dividirla en artÃ­culos y generar una base estructurada.
3. Crear una capa de bÃºsqueda semÃ¡ntica mediante **embeddings gratuitos**.
4. Recuperar los artÃ­culos relevantes ante cada consulta.
5. Generar respuestas explicadas y citadas mediante un **LLM open-source**.
6. Ofrecer una **interfaz interactiva tipo chatbot**.
7. Registrar mÃ©tricas y logs para un mÃ³dulo de **observabilidad del LLM**.

---

## ğŸ§  TecnologÃ­as principales

- **LLM gratuito**: Llama 3.1 (8B) ejecutÃ¡ndose localmente con **Ollama**.
- **Embeddings gratuitos**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`.
- **BÃºsqueda semÃ¡ntica**: FAISS (Ã­ndice vectorial local).
- **PLN y pipeline RAG**: LangChain.
- **OrquestaciÃ³n del agente**: LangGraph.
- **Interfaz del chatbot**: Streamlit.
- **ExtracciÃ³n de informaciÃ³n del PDF**: pdfplumber.
- **Lenguaje**: Python 3.10+.
- **Observabilidad**: Logs estructurados (tiempo, latencia, artÃ­culos recuperados, feedback).

Todo el proyecto funciona **100% offline y sin costos**, gracias al uso de modelos y herramientas open-source.

---

```mermaid
flowchart TD

    A["Usuario"] --> B["Interfaz Chatbot\n(Streamlit)"]

    B --> C["Agente en LangGraph"]

    C -->|Router| D{"Â¿Pregunta sobre trÃ¡nsito?"}

    D -->|No| Z1["Respuesta:\nPregunta fuera del dominio"]
    D -->|SÃ­| E["Retriever Node\n(FAISS + Embeddings)"]

    E -->|Top-k artÃ­culos| F["RAG LLM Node\n(Llama 3.1 / Ollama)"]

    F --> G["Validador\n(VerificaciÃ³n de citas)"]
    G --> H["Logger Node\n(Observabilidad)"]

    H --> I["Respuesta Final\n(Texto + ArtÃ­culos usados)"]
    I --> B

    E --- J["Ãndice Vectorial\nFAISS"]
    E --- K["Embeddings\nSentence-Transformers"]

    K --- L["ArtÃ­culos Procesados\nLey 769"]
    L --- M["PDF Original Ley 769\nProcesado con pdfplumber"]
```



## ğŸ“ Estructura del Proyecto

```text
pln-transito-chatbot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # PDF original de la Ley 769
â”‚   â”œâ”€â”€ interim/      # Texto extraÃ­do sin procesar
â”‚   â””â”€â”€ processed/    # ArtÃ­culos limpios y base estructurada para RAG
â”œâ”€â”€ src/              # CÃ³digo del procesamiento, embeddings, Ã­ndice y agente
â”œâ”€â”€ app/              # Interfaz del chatbot en Streamlit
â”œâ”€â”€ logs/             # Observabilidad del LLM
â”œâ”€â”€ report/           # Informe final del proyecto
â””â”€â”€ README.md

## ğŸ“˜ PASO 3 â€” ConstrucciÃ³n de Embeddings y CreaciÃ³n del Ãndice FAISS

Una vez procesada la Ley 769 de 2002 y convertida en una tabla estructurada de 180 artÃ­culos, el siguiente paso fue habilitar la bÃºsqueda semÃ¡ntica mediante **RAG (Retrieval-Augmented Generation)**.  
Para ello se construyeron:

1. **Embeddings** (representaciones vectoriales) para cada artÃ­culo.  
2. Un **Ã­ndice FAISS** que permite realizar bÃºsquedas rÃ¡pidas y eficientes sobre esos vectores.

Este paso es fundamental para que el agente pueda recuperar los artÃ­culos mÃ¡s relevantes ante cualquier pregunta del usuario.

---

### ğŸ”¹ 1. Modelo de Embeddings (Gratis y Local)

Se utilizÃ³ un modelo gratuito y open-source de `sentence-transformers`:

- **paraphrase-multilingual-MiniLM-L12-v2**

Ventajas:

- MultilingÃ¼e (incluye espaÃ±ol)
- Ligero y rÃ¡pido
- No requiere GPU
- Funciona completamente offline
- Excelente desempeÃ±o para tareas de recuperaciÃ³n semÃ¡ntica

El modelo transforma cada artÃ­culo en un vector de dimensiÃ³n fija (384 dimensiones).

---

### ğŸ”¹ 2. Embeddings generados

Para los 180 artÃ­culos de la Ley 769 se generaron 180 embeddings, almacenados en:


Cada embedding es un vector representativo del contenido del artÃ­culo, permitiendo medir similitud semÃ¡ntica entre la pregunta del usuario y el texto legal.

---

### ğŸ”¹ 3. ConstrucciÃ³n del Ãndice FAISS

Con los embeddings ya generados, se creÃ³ un Ã­ndice vectorial utilizando **FAISS (Facebook AI Similarity Search)**, una herramienta especializada para realizar bÃºsquedas por similitud de manera eficiente.

El Ã­ndice permite:

- Recuperar los artÃ­culos mÃ¡s similares a una pregunta
- Consultas rÃ¡pidas incluso con miles de vectores
- Filtrar artÃ­culos relevantes como primer paso del RAG

El Ã­ndice quedÃ³ guardado en:


---

### ğŸ”¹ 4. Resultado del proceso

âœ” El modelo de embeddings se cargÃ³ correctamente  
âœ” Se generaron embeddings para los 180 artÃ­culos  
âœ” Se construyÃ³ el Ã­ndice FAISS  
âœ” Todos los archivos quedaron guardados en `data/processed/models/`

Este paso deja lista la infraestructura de recuperaciÃ³n para el siguiente mÃ³dulo:  
**el RAG simple**, donde el sistema serÃ¡ capaz de:

- Tomar una pregunta del usuario
- Convertirla en embedding
- Buscar los artÃ­culos mÃ¡s relacionados
- Retornar el top-k para generar una respuesta con Llama 3.1

---

### ğŸ“Œ QuÃ© sigue

En el siguiente paso se implementarÃ¡ el **RAG Simple**, que permitirÃ¡:

- Consultar artÃ­culos relevantes
- Enviar el contexto al LLM (Llama 3.1 â€“ Ollama)
- Generar respuestas legales claras y citadas


ğŸ§  Paso 4 â€” ConstrucciÃ³n del RAG Simple
En esta fase se implementÃ³ la primera versiÃ³n del sistema RAG (Retrieval-Augmented Generation), utilizando los artÃ­culos procesados de la Ley 769, los embeddings locales y el Ã­ndice FAISS previamente construido.
El flujo del RAG simple es:
El usuario hace una pregunta sobre trÃ¡nsito.
El sistema genera embeddings de la pregunta.
Se busca en el Ã­ndice FAISS los artÃ­culos mÃ¡s similares (top-k).
Se arma un prompt contextual con los artÃ­culos recuperados.
El LLM Llama 3.1 (ejecutÃ¡ndose localmente en Ollama) genera la respuesta:
clara
explicada en espaÃ±ol
citando artÃ­culos relevantes
Se muestran tanto la respuesta como los artÃ­culos usados.
Este RAG simple sirve como la base para el agente mÃ¡s avanzado que se construirÃ¡ en LangGraph en los siguientes pasos.
Permite validar que todo el pipeline datos â†’ embeddings â†’ FAISS â†’ LLM estÃ© funcionando correctamente.
âœ”ï¸ Ejemplo real ejecutado:
Pregunta: Â¿QuÃ© pasa si me paso un semÃ¡foro en rojo?

Respuesta del sistema:
SegÃºn la Ley 769 de 2002, artÃ­culo 118...
[textoâ€¦]

ArtÃ­culos usados:
118, 65, 90
âœ”ï¸ Componentes del RAG Simple
Embeddings: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
Ãndice vectorial: FAISS (local)
Modelo generador: Llama 3.1 (vÃ­a Ollama)
Pila tecnolÃ³gica: Python + LangChain
ğŸ“Œ Objetivo del paso
Validar que la bÃºsqueda semÃ¡ntica y la generaciÃ³n contextual funcionan correctamente antes de crear el Agente en LangGraph.

## ğŸ§© Paso 5 â€” ConstrucciÃ³n de los Nodos del Agente (LangGraph)

En este paso comenzamos la transiciÃ³n desde un sistema RAG simple hacia un **Agente completo basado en LangGraph**, capaz de razonar sobre el flujo de la conversaciÃ³n y ejecutar acciones estructuradas.

Para ello, creamos el mÃ³dulo `nodes.py`, que contiene los **nodos fundamentales del agente**, cada uno representando una operaciÃ³n clave en el pipeline:

### ğŸ”¹ 1. Router Node
Determina si la pregunta del usuario pertenece o no al dominio del agente (Ley 769 de 2002 - trÃ¡nsito colombiano).  
Si la pregunta no estÃ¡ relacionada, el nodo genera una respuesta adecuada fuera del dominio.

### ğŸ”¹ 2. Retriever Node
Se encarga de buscar los artÃ­culos relevantes utilizando:
- el Ã­ndice FAISS creado en el paso anterior  
- los embeddings generados con Sentence-Transformers  

El nodo retorna el conjunto top-k de artÃ­culos mÃ¡s similares.

### ğŸ”¹ 3. RAG LLM Node
Genera una respuesta usando:
- el contexto recuperado por FAISS  
- el modelo Llama 3.1 (ejecutado localmente con Ollama)

Este nodo produce una respuesta completa en lenguaje natural, citando los artÃ­culos relevantes.

### ğŸ”¹ 4. Validador Node
Verifica que la respuesta del LLM:
- cite artÃ­culos reales  
- no incluya informaciÃ³n fuera del contexto recuperado  
- no presente alucinaciones obvias  

En caso necesario, puede solicitar un reintento o ajustar la respuesta.

### ğŸ”¹ 5. Logger Node
Registra la interacciÃ³n del usuario para el mÃ³dulo de observabilidad:
- pregunta  
- artÃ­culos recuperados  
- respuesta generada  
- latencia  
- feedback del usuario (si aplica)

Los logs se guardan en `logs/` para anÃ¡lisis posterior.

---

En conjunto, estos nodos componen el **corazÃ³n del Agente RAG**, permitiendo un flujo robusto, modular y monitoreable, listo para conectarse en el siguiente paso al grafo completo de LangGraph.
